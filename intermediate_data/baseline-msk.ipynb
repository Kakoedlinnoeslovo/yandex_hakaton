{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple baseline\n",
    "\n",
    "In this baseline we take data from the primary source, aggregate it by squares and compute some basic features from those squares.\n",
    "\n",
    "We then fit a gradient boosting ensemble to predict whether it was raining in this particular square & hour.\n",
    "\n",
    "For starters, let's take a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../data/train_msk.tsv\"\n",
    "NETATMO_PATH = \"../data/train_msk_netatmo.tsv\"\n",
    "TEST_PATH = \"../data/test_msk_features.tsv\"\n",
    "TEST_NETATMO_PATH = \"../data/test_msk_netatmo.tsv\"\n",
    "CITY_PREDICTIONS_PATH = \"../intermediate_data/prediction_msk.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(TRAIN_PATH, sep='\\t',dtype=json.load(open(\"../data/train_col_dtypes.json\")), chunksize=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x1,y1,x2,y2):\n",
    "    return np.linalg.norm(x1-x2) + np.linalg.norm(y2-y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "16it [01:37,  6.97s/it]\n"
     ]
    }
   ],
   "source": [
    "temp = pd.DataFrame()\n",
    "i = np.random.rand(0,10)\n",
    "j=0\n",
    "train = pd.DataFrame()\n",
    "temp_chunk=pd.DataFrame()\n",
    "for chunk in tqdm(chunks):\n",
    "        temp_chunk = chunk\n",
    "        temp['city_code']=chunk['city_code']\n",
    "        temp['sq_x']=chunk['sq_x']\n",
    "        temp['sq_y']=chunk['sq_y']\n",
    "        temp['hour_hash']=chunk['hour_hash']\n",
    "\n",
    "        temp['sqr_dist'] = dist(chunk['sq_lat'],chunk['sq_lon'],\n",
    "                                chunk['cell_lat'],chunk['cell_lon'])\n",
    "\n",
    "        temp['usr_dist'] = dist(chunk['ulat'], chunk['ulon'], \n",
    "                                chunk['cell_lat'],chunk['cell_lon'])\n",
    "        temp['cell_range'] = chunk[chunk['range']>-100]['range'].mean()\n",
    "\n",
    "        temp_hash = str(chunk['u_hashed'])+ str(chunk['ver_hash'])+ str(chunk['device_model_hash'])\n",
    "        temp['usr_hash'] = abs(hash(temp_hash)) % (10 ** 8)\n",
    "\n",
    "        temp['sig_Strength'] = chunk['SignalStrength'] - chunk['SignalStrength'].mean()\n",
    "\n",
    "        temp['sig'] = 0.1*chunk['LocationDirection']+0.1*chunk['LocationPrecision']+0.1* chunk['LocationSpeed']\n",
    "        temp['rain'] = chunk['rain']\n",
    "        temp['hours_since'] = chunk['hours_since']\n",
    "        train = train.append(temp, ignore_index=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500000, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('my_train_msc.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('my_train_msc.csv').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[\"city_code\",\"sq_x\",\"sq_y\",\"hour_hash\"]\n",
    "#SignalStrength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=[12,6])\n",
    "# plt.subplot(1,2,1)\n",
    "# plt.scatter(train['ulon'][::100],train['ulat'][::100],alpha=0.01,marker='.')\n",
    "# plt.subplot(1,2,2)\n",
    "# plt.hist(train['precipitation'][::100],range=[0,5],bins=30)\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel(\"precipitation strength\")\n",
    "# plt.show()\n",
    "\n",
    "# train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ if you're low on memory, try this:\n",
    "* Most obviously, downsample data\n",
    "* Read one square at a time: read it, compute features, and only then read next square\n",
    "* Entries for each cell appear as subsequent rows in the dataset, so you can just read, say, 25% of the data and process it, then go for next 25%, etc.\n",
    "* Delete training data and intermediate aggregations liky `groupby` after you've done with feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with netatmo\n",
    "\n",
    "Customer grade meteostations are excellent sources of data on rain. Alas, they're rather scarce and we're unlikely to find stations in every square/time block. Therefore we're gonna need to quickly find ones from neighboring blocks.\n",
    "\n",
    "For performance reasons, we'll use fast nearest neighbor lookup methods from sklearn.\n",
    "Note that those are not the fastest neighbor lookup methods available, but they should be enough for the baseline.\n",
    "\n",
    "We'll query the users that have neighboring longitude/lattitude within this hour. In this baseline we implicitly compute euclidian distance over latitude/longitude axes which has a number of problems: the distance gets larger as you move from equator to the poles. More importantly, this method does not take adjacent hours into consideration.\n",
    "You are invited to improve on those points in your solution :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 11)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "def preprocess_netatmo(df):\n",
    "    \"\"\"organizes netatmo stations into KDTrees for each distinct time frame\"\"\"\n",
    "    \n",
    "    df_by_hour = df.groupby('hour_hash')\n",
    "    anns = {}\n",
    "    for hour,stations_group in df_by_hour:\n",
    "        anns[hour] = KDTree(stations_group[[\"netatmo_latitude\",\"netatmo_longitude\"]].values,metric='minkowski',p=2)\n",
    "    \n",
    "    #convert groupby to dict to get faster queries\n",
    "    df_by_hour = {group:stations_group for group,stations_group in df_by_hour}\n",
    "    \n",
    "    return df_by_hour,anns\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "netatmo_groups,netatmo_anns = preprocess_netatmo(pd.read_csv(NETATMO_PATH,na_values=\"None\",sep='\\t',dtype={'hour_hash':\"uint64\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "In this baseline, we're going to aggregate all user data from a specific square and a specific hour to predict whether it's raining in this square. We'll split data into blocks by `[sq_lon,sq_lat,sq_time]` and process such blocks independently.\n",
    "\n",
    "<img src=\"https://usercontent1.hubstatic.com/12943886_f520.jpg\" width=240px>\n",
    "\n",
    "\n",
    "The next cell defines a function that extracts features from such blocks. Feel free to add some new features here or drop those you believe to be harmful.\n",
    "\n",
    "Also note that this isn't the only way to process such data. See the [known unknowns](#known_unknowns) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(x1,y1,x2,y2):\n",
    "    return np.linalg.norm(x1-x2) + np.linalg.norm(y2-y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(group,netatmo_groups,netatmo_anns):\n",
    "    \"\"\"\n",
    "    Extracts all kinds of features from a dataframe containing users in one group\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    #square features\n",
    "    square = {col: group[col].iloc[0] for col in group.columns}\n",
    "    \n",
    "#     features['square_lat'] = square['sq_lat']\n",
    "#     features['square_lon'] = square['sq_lon']\n",
    "#     features['time_of_day'] = square['day_hour']\n",
    "\n",
    "#     #signal strength\n",
    "#     features['signal_mean'] = group['SignalStrength'] - group['SignalStrength'].mean(axis=0)\n",
    "#     features['rain'] = group['rain']\n",
    "    \n",
    "\n",
    "#     #features for each user\n",
    "#     group_by_user = group.groupby('u_hashed')\n",
    "#     group_by_user.apply(lambda group: group['ulat'].var()+group['ulon'].var())\n",
    "    \n",
    "#     features['num_users'] = len(group_by_user)\n",
    "#     features['mean_entries_per_user'] = group_by_user.apply(len).mean()\n",
    "#     features['mean_user_signal_var'] = group_by_user.apply(\n",
    "#         lambda user_entries: user_entries['SignalStrength'].var()).mean()\n",
    "    \n",
    "#     #netatmo features\n",
    "#     if square['hour_hash'] in netatmo_groups:\n",
    "#         local_stations,neighbors = netatmo_groups[square['hour_hash']],netatmo_anns[square['hour_hash']]\n",
    "#         [distances],[neighbor_ids] = neighbors.query([(square['sq_lat'],square['sq_lon'])],k=10)\n",
    "\n",
    "#         neighbor_stations = local_stations.iloc[neighbor_ids]\n",
    "\n",
    "#         features['distance_to_closest_station'] = np.min(distances)\n",
    "#         features['mean_distance_to_station'] = np.mean(distances)\n",
    "\n",
    "#         for colname in ['netatmo_pressure_mbar','netatmo_temperature_c','netatmo_sum_rain_24h',\n",
    "#                         'netatmo_humidity_percent',\"netatmo_wind_speed_kmh\",\"netatmo_wind_gust_speed_kmh\"]:\n",
    "#             col = neighbor_stations[colname].dropna()\n",
    "#             if len(col)!=0:\n",
    "#                 features[colname+\"_mean\"],features[colname+\"_std\"] = col.mean(),col.var()\n",
    "#             else:\n",
    "#                 features[colname+\"_mean\"],features[colname+\"_std\"] = np.nan,np.nan\n",
    "\n",
    "#     return features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply it to all the squares we have.\n",
    "\n",
    "This may take time, more so if you use complex features, so you can try to speed stuff up by using [parallel groupby-apply](https://stackoverflow.com/a/27027632) or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "groupby = train.groupby([\"city_code\",\"sq_x\",\"sq_y\",\"hour_hash\"])\n",
    "\n",
    "X,y,block_ids = [],[],[]\n",
    "\n",
    "for block_id in tqdm(groupby.groups):\n",
    "    group = groupby.get_group(block_id)\n",
    "    X.append(group,netatmo_groups,netatmo_anns)\n",
    "    y.append(group.iloc[0]['rain'])\n",
    "    block_ids.append(block_id+(group.iloc[0][\"hours_since\"],))\n",
    "\n",
    "X = pd.DataFrame(X).fillna(-999.)\n",
    "y = np.array(y)\n",
    "block_ids = pd.DataFrame(block_ids,columns=[\"city_code\",\"sq_x\",\"sq_y\",\"hour_hash\",\"hours_since\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ If you're low on memory, it's time to either delete train & groupby or pickle X/y/block_ids and restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "Once the data is processed, it's time to train some machine learning model that would predict rain given all features we gathered.\n",
    "\n",
    "Since our features are all of different nature and unit scale (hours,decibels,degrees,etc.), it makes sense to use decision tree-based methods to for classification.\n",
    "\n",
    "\n",
    "<img src=\"http://zdnet2.cbsistatic.com/hub/i/2017/07/18/d3f47c3e-8529-4855-a0e1-c686ee3b4007/d1113adf74bb59c3b46419a531c39c3e/orig.png\" width=320>\n",
    "In particular, we apply [CatBoost](https://catboost.yandex/), Yandex' recent open source gradient boosting implementation.\n",
    "\n",
    "To make this baseline simple, we use catboost with default settings. You can certainly find a better combination of parameters. \n",
    "\n",
    "Here's a [guide](https://tech.yandex.com/catboost/doc/dg/concepts/parameter-tuning-docpage/) on how catboost hyperparameters work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train = block_ids['hours_since'] <= np.percentile(block_ids['hours_since'],85) #leave last 15% for validation\n",
    "\n",
    "X_train,y_train = X[in_train],y[in_train]\n",
    "X_val,y_val = X[~in_train],y[~in_train]\n",
    "print(\"Training samples: %i; Validation samples: %i\"%(len(X_train),len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #if you don't have catboost installed, use !pip install catboost\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "# model = CatBoostClassifier().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBClassifier(max_depth=3,n_estimators=200)\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "# Get predicted classes\n",
    "preds_class = model.predict(X_test)\n",
    "# Get predicted probabilities for each class\n",
    "preds_proba = model.predict_proba(X_test)\n",
    "# Get predicted RawFormulaVal\n",
    "#preds_raw = model.predict(test_data, prediction_type='RawFormulaVal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_data, train_labels, cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBClassifier(max_depth=3,n_estimators=200)\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "# Get predicted classes\n",
    "preds_class = model.predict(X_test)\n",
    "# Get predicted probabilities for each class\n",
    "preds_proba = model.predict_proba(X_test)\n",
    "# Get predicted RawFormulaVal\n",
    "#preds_raw = model.predict(test_data, prediction_type='RawFormulaVal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing results\n",
    "\n",
    "Here you can see importances of all individual features, ranked from worst to best.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "y_train_pred = model.predict_proba(X_train)[:,1]\n",
    "print(\"Train ROC AUC:\",roc_auc_score(y_train,y_train_pred))\n",
    "\n",
    "fpr,tpr,_ = roc_curve(y_train, y_train_pred)\n",
    "plt.plot(fpr,tpr,label='train AUC')\n",
    "\n",
    "y_val_pred = model.predict_proba(X_val)[:,1]\n",
    "print(\"Val ROC AUC:\",roc_auc_score(y_val,y_val_pred))\n",
    "\n",
    "fpr,tpr,_ = roc_curve(y_val, y_val_pred)\n",
    "plt.plot(fpr,tpr,label='validation AUC')\n",
    "\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(model._feature_importance)\n",
    "plt.figure(figsize=[6,9])\n",
    "plt.plot(np.array(model._feature_importance)[order],range(len(order)),marker='o')\n",
    "plt.hlines(range(len(order)),np.zeros_like(order),np.array(model._feature_importance)[order],linestyles=':')\n",
    "plt.yticks(range(X.shape[1]),X.columns[order]);\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.xlim([0.1,max(model._feature_importance)*1.5])\n",
    "plt.ylim(-1,len(order))\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Final model and uploading the results\n",
    "\n",
    "\n",
    "Competition data contains three cities: Moscow, Saint-Petersburg and Kazan. To submit a prediction, you'll have to run this baseline three times separately for each city and concatenate the results. \n",
    "\n",
    "The code assumes that you ran this solution for each city (see comments below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model on full data. Copy model definition here.\n",
    "model = CatBoostClassifier().fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv(TEST_PATH, sep='\\t',dtype=json.load(open(\"./data/test_col_dtypes.json\")),)\n",
    "test_groupby = test.groupby([\"city_code\",\"sq_x\",\"sq_y\",\"hour_hash\"])\n",
    "test_netatmo_groups,test_netatmo_anns = preprocess_netatmo(pd.read_csv(TEST_NETATMO_PATH,na_values=\"None\",\n",
    "                                                                       sep='\\t',dtype={'hour_hash':\"uint64\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,test_block_ids = [],[]\n",
    "for block_id in tqdm(test_groupby.groups):\n",
    "    group = test_groupby.get_group(block_id)\n",
    "    X_test.append(extract_features(group,test_netatmo_groups,test_netatmo_anns))\n",
    "    test_block_ids.append(block_id)\n",
    "    \n",
    "X_test = pd.DataFrame(X_test)\n",
    "test_block_ids = pd.DataFrame(test_block_ids,columns=[\"city_code\",\"sq_x\",\"sq_y\",\"hour_hash\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code saves the prediction for one city.\n",
    "prediction_for_one_city = test_block_ids.copy()\n",
    "prediction_for_one_city[\"prediction\"] = model.predict_proba(X_test)[:,1]\n",
    "prediction_for_one_city.to_csv(CITY_PREDICTIONS_PATH)\n",
    "\n",
    "prediction_for_one_city.head()\n",
    "\n",
    "#WARNING! you must run this notebook for all three regions before proceeding!\n",
    "#We assume that you have prediction_msk.csv , prediction_spb.csv and prediction_kazan.csv files prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X.copy()\n",
    "data[\"target\"] = y\n",
    "data.to_csv(\"intermediate_data/msk.csv\")\n",
    "X_test.to_csv(\"intermediate_data/msk_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all predictions and make submission file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ```\n",
    " \n",
    " ```\n",
    "\n",
    "![img](https://images-na.ssl-images-amazon.com/images/I/31la29lBQxL.jpg)\n",
    "\n",
    "\n",
    " ```\n",
    " \n",
    " ```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
