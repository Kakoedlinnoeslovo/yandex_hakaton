{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple baseline\n",
    "\n",
    "In this baseline we take data from the primary source, aggregate it by squares and compute some basic features from those squares.\n",
    "\n",
    "We then fit a gradient boosting ensemble to predict whether it was raining in this particular square & hour.\n",
    "\n",
    "For starters, let's take a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./data/train_spb.tsv\"\n",
    "NETATMO_PATH = \"./data/train_spb_netatmo.tsv\"\n",
    "TEST_PATH = \"./data/test_spb_features.tsv\"\n",
    "TEST_NETATMO_PATH = \"./data/test_spb_netatmo.tsv\"\n",
    "\n",
    "CITY_PREDICTIONS_PATH = \"./intermediate_data/prediction_spb.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read (pandas/_libs/parsers.c:10862)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory (pandas/_libs/parsers.c:11138)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows (pandas/_libs/parsers.c:12175)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data (pandas/_libs/parsers.c:14136)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens (pandas/_libs/parsers.c:14658)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype (pandas/_libs/parsers.c:15629)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m     \"\"\"\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e6046124b0b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAIN_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/train_col_dtypes.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1748\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1749\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1750\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_PATH, sep='\\t',dtype=json.load(open(\"./data/train_col_dtypes.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12,6])\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(train['ulon'][::100],train['ulat'][::100],alpha=0.01,marker='.')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(train['precipitation'][::100],range=[0,5],bins=30)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"precipitation strength\")\n",
    "plt.show()\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ if you're low on memory, try this:\n",
    "* Most obviously, downsample data\n",
    "* Read one square at a time: read it, compute features, and only then read next square\n",
    "* Entries for each cell appear as subsequent rows in the dataset, so you can just read, say, 25% of the data and process it, then go for next 25%, etc.\n",
    "* Delete training data and intermediate aggregations liky `groupby` after you've done with feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with netatmo\n",
    "\n",
    "Customer grade meteostations are excellent sources of data on rain. Alas, they're rather scarce and we're unlikely to find stations in every square/time block. Therefore we're gonna need to quickly find ones from neighboring blocks.\n",
    "\n",
    "For performance reasons, we'll use fast nearest neighbor lookup methods from sklearn.\n",
    "Note that those are not the fastest neighbor lookup methods available, but they should be enough for the baseline.\n",
    "\n",
    "We'll query the users that have neighboring longitude/lattitude within this hour. In this baseline we implicitly compute euclidian distance over latitude/longitude axes which has a number of problems: the distance gets larger as you move from equator to the poles. More importantly, this method does not take adjacent hours into consideration.\n",
    "You are invited to improve on those points in your solution :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "def preprocess_netatmo(df):\n",
    "    \"\"\"organizes netatmo stations into KDTrees for each distinct time frame\"\"\"\n",
    "    \n",
    "    df_by_hour = df.groupby('hour_hash')\n",
    "    anns = {}\n",
    "    for hour,stations_group in df_by_hour:\n",
    "        anns[hour] = KDTree(stations_group[[\"netatmo_latitude\",\"netatmo_longitude\"]].values,metric='minkowski',p=2)\n",
    "    \n",
    "    #convert groupby to dict to get faster queries\n",
    "    df_by_hour = {group:stations_group for group,stations_group in df_by_hour}\n",
    "    \n",
    "    return df_by_hour,anns\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netatmo_groups,netatmo_anns = preprocess_netatmo(pd.read_csv(NETATMO_PATH,na_values=\"None\",sep='\\t',dtype={'hour_hash':\"uint64\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "In this baseline, we're going to aggregate all user data from a specific square and a specific hour to predict whether it's raining in this square. We'll split data into blocks by `[sq_lon,sq_lat,sq_time]` and process such blocks independently.\n",
    "\n",
    "<img src=\"https://usercontent1.hubstatic.com/12943886_f520.jpg\" width=240px>\n",
    "\n",
    "\n",
    "The next cell defines a function that extracts features from such blocks. Feel free to add some new features here or drop those you believe to be harmful.\n",
    "\n",
    "Also note that this isn't the only way to process such data. See the [known unknowns](#known_unknowns) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(group,netatmo_groups,netatmo_anns):\n",
    "    \"\"\"\n",
    "    Extracts all kinds of features from a dataframe containing users in one group\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    #square features\n",
    "    square = {col: group[col].iloc[0] for col in group.columns}\n",
    "    \n",
    "    features['square_lat'] = square['sq_lat']\n",
    "    features['square_lon'] = square['sq_lon']\n",
    "    features['time_of_day'] = square['day_hour']\n",
    "\n",
    "    #signal strength\n",
    "    features['signal_mean'] = group['SignalStrength'].mean()\n",
    "    features['signal_var'] = group['SignalStrength'].var()\n",
    "\n",
    "    #features for each user\n",
    "    group_by_user = group.groupby('u_hashed')\n",
    "    group_by_user.apply(lambda group: group['ulat'].var()+group['ulon'].var())\n",
    "    \n",
    "    features['num_users'] = len(group_by_user)\n",
    "    features['mean_entries_per_user'] = group_by_user.apply(len).mean()\n",
    "    features['mean_user_signal_var'] = group_by_user.apply(\n",
    "        lambda user_entries: user_entries['SignalStrength'].var()).mean()\n",
    "    \n",
    "    #netatmo features\n",
    "    if square['hour_hash'] in netatmo_groups:\n",
    "        local_stations,neighbors = netatmo_groups[square['hour_hash']],netatmo_anns[square['hour_hash']]\n",
    "        [distances],[neighbor_ids] = neighbors.query([(square['sq_lat'],square['sq_lon'])],k=10)\n",
    "\n",
    "        neighbor_stations = local_stations.iloc[neighbor_ids]\n",
    "\n",
    "        features['distance_to_closest_station'] = np.min(distances)\n",
    "        features['mean_distance_to_station'] = np.mean(distances)\n",
    "\n",
    "        for colname in ['netatmo_pressure_mbar','netatmo_temperature_c','netatmo_sum_rain_24h',\n",
    "                        'netatmo_humidity_percent',\"netatmo_wind_speed_kmh\",\"netatmo_wind_gust_speed_kmh\"]:\n",
    "            col = neighbor_stations[colname].dropna()\n",
    "            if len(col)!=0:\n",
    "                features[colname+\"_mean\"],features[colname+\"_std\"] = col.mean(),col.var()\n",
    "            else:\n",
    "                features[colname+\"_mean\"],features[colname+\"_std\"] = np.nan,np.nan\n",
    "\n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply it to all the squares we have.\n",
    "\n",
    "This may take time, more so if you use complex features, so you can try to speed stuff up by using [joblib.Parallel](http://pythonhosted.org/joblib/parallel.html) or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "groupby = train.groupby([\"city_code\",\"sq_x\",\"sq_y\",\"hour_hash\"])\n",
    "\n",
    "X,y,block_ids = [],[],[]\n",
    "\n",
    "for block_id in tqdm(groupby.groups):\n",
    "    group = groupby.get_group(block_id)\n",
    "    X.append(extract_features(group,netatmo_groups,netatmo_anns))\n",
    "    y.append(group.iloc[0]['rain'])\n",
    "    block_ids.append(block_id+(group.iloc[0][\"hours_since\"],))\n",
    "\n",
    "X = pd.DataFrame(X).fillna(-999.)\n",
    "y = np.array(y)\n",
    "block_ids = pd.DataFrame(block_ids,columns=[\"city_code\",\"sq_x\",\"sq_y\",\"hour_hash\",\"hours_since\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ If you're low on memory, it's time to either delete train & groupby or pickle X/y/block_ids and restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "Once the data is processed, it's time to train some machine learning model that would predict rain given all features we gathered.\n",
    "\n",
    "Since our features are all of different nature and unit scale (hours,decibels,degrees,etc.), it makes sense to use decision tree-based methods to for classification.\n",
    "\n",
    "\n",
    "<img src=\"http://zdnet2.cbsistatic.com/hub/i/2017/07/18/d3f47c3e-8529-4855-a0e1-c686ee3b4007/d1113adf74bb59c3b46419a531c39c3e/orig.png\" width=320>\n",
    "In particular, we apply [CatBoost](https://catboost.yandex/), Yandex' recent open source gradient boosting implementation.\n",
    "\n",
    "To make this baseline simple, we use catboost with default settings. You can certainly find a better combination of parameters. \n",
    "\n",
    "Here's a [guide](https://tech.yandex.com/catboost/doc/dg/concepts/parameter-tuning-docpage/) on how catboost hyperparameters work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_train = block_ids['hours_since'] <= np.percentile(block_ids['hours_since'],85) #leave last 15% for validation\n",
    "\n",
    "X_train,y_train = X[in_train],y[in_train]\n",
    "X_val,y_val = X[~in_train],y[~in_train]\n",
    "print(\"Training samples: %i; Validation samples: %i\"%(len(X_train),len(X_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you don't have catboost installed, use !pip install catboost\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing results\n",
    "\n",
    "Here you can see importances of all individual features, ranked from worst to best.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score,roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "y_train_pred = model.predict_proba(X_train)[:,1]\n",
    "print(\"Train ROC AUC:\",roc_auc_score(y_train,y_train_pred))\n",
    "\n",
    "fpr,tpr,_ = roc_curve(y_train, y_train_pred)\n",
    "plt.plot(fpr,tpr,label='train AUC')\n",
    "\n",
    "y_val_pred = model.predict_proba(X_val)[:,1]\n",
    "print(\"Val ROC AUC:\",roc_auc_score(y_val,y_val_pred))\n",
    "\n",
    "fpr,tpr,_ = roc_curve(y_val, y_val_pred)\n",
    "plt.plot(fpr,tpr,label='validation AUC')\n",
    "\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = np.argsort(model._feature_importance)\n",
    "plt.figure(figsize=[6,9])\n",
    "plt.plot(np.array(model._feature_importance)[order],range(len(order)),marker='o')\n",
    "plt.hlines(range(len(order)),np.zeros_like(order),np.array(model._feature_importance)[order],linestyles=':')\n",
    "plt.yticks(range(X.shape[1]),X.columns[order]);\n",
    "plt.tick_params(labelsize=16)\n",
    "plt.xlim([0.1,max(model._feature_importance)*1.5])\n",
    "plt.ylim(-1,len(order))\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Final model and uploading the results\n",
    "\n",
    "\n",
    "Competition data contains three cities: Moscow, Saint-Petersburg and Kazan. To submit a prediction, you'll have to run this baseline three times separately for each city and concatenate the results. \n",
    "\n",
    "The code assumes that you ran this solution for each city (see comments below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model on full data. Copy model definition here.\n",
    "\n",
    "model = CatBoostClassifier().fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = pd.read_csv(TEST_PATH, sep='\\t',dtype=json.load(open(\"./data/test_col_dtypes.json\")),)\n",
    "test_groupby = test.groupby([\"city_code\",\"sq_x\",\"sq_y\",\"hour_hash\"])\n",
    "test_netatmo_groups,test_netatmo_anns = preprocess_netatmo(pd.read_csv(TEST_NETATMO_PATH,na_values=\"None\",\n",
    "                                                                       sep='\\t',dtype={'hour_hash':\"uint64\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,test_block_ids = [],[]\n",
    "for block_id in tqdm(test_groupby.groups):\n",
    "    group = test_groupby.get_group(block_id)\n",
    "    X_test.append(extract_features(group,test_netatmo_groups,test_netatmo_anns))\n",
    "    test_block_ids.append(block_id)\n",
    "    \n",
    "X_test = pd.DataFrame(X_test)\n",
    "test_block_ids = pd.DataFrame(test_block_ids,columns=[\"city_code\",\"sq_x\",\"sq_y\",\"hour_hash\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code saves the prediction for one city.\n",
    "prediction_for_one_city = test_block_ids.copy()\n",
    "prediction_for_one_city[\"prediction\"] = model.predict_proba(X_test)[:,1]\n",
    "prediction_for_one_city.to_csv(CITY_PREDICTIONS_PATH)\n",
    "\n",
    "prediction_for_one_city.head()\n",
    "\n",
    "#WARNING! you must run this notebook for all three regions before proceeding!\n",
    "#We assume that you have prediction_msk.csv , prediction_spb.csv and prediction_kazan.csv files prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X.copy()\n",
    "data[\"target\"] = y\n",
    "data.to_csv(\"intermediate_data/spb.csv\")\n",
    "X_test.to_csv(\"intermediate_data/spb_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather all predictions and make submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "predictions = pd.concat(\n",
    "    [pd.read_csv(fname,index_col=0) for fname in (\"./intermediate_data/prediction_kazan.csv\",\n",
    "                                                  \"./intermediate_data/prediction_spb.csv\",\n",
    "                                                  \"./intermediate_data/prediction_msk.csv\")],\n",
    "    ignore_index=True\n",
    ")\n",
    "blocks = pd.read_csv(\"./data/hackathon_tosubmit.tsv\",sep='\\t')\n",
    "assert len(predictions) == len(blocks),\"Predictions don't match blocks. Sumbit at your own risk.\"\n",
    "\n",
    "merged = pd.merge(blocks,predictions,how='left',on=[\"sq_x\",\"sq_y\",\"hour_hash\"])\n",
    "assert not np.isnan(merged.prediction).any(), \"some predictions are missing. Sumbit at your own risk.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged[['id','prediction']].to_csv(\"baseline_submission.csv\",sep=',',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now upload baseline_submission.csv to the competition interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head baseline_submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known unknowns <a id='known_unknowns'>\n",
    "\n",
    "Here's a few ideas to improve your solution:\n",
    "* Right now we only consider users in the same square where we're going to make prediction.\n",
    " * It may be useful to consider neighboring squares in square id and/or time\n",
    " * It may be useful to use global city-wide estimate (like \"There's currently no rain in Moscow\")\n",
    " * Same is true for netatmo stations\n",
    "* There's a lot of underexplored features\n",
    " * Netatmo stations' features\n",
    " * User behavior on device level, e.g. \"phone signal worse than usual\"\n",
    " * Latitude/longitude are fed to model in \n",
    " * Relations between several  kinds of features (e.g. signal over distance to cell)\n",
    " * Relations over location/time, e.g. \"less users than usual\"\n",
    "* Data splits\n",
    " * Test set rains may be more/less frequent than on the training set\n",
    " * There also may be some difference in user activity\n",
    " * There's definitely a difference in distribution of users and stations in different cities\n",
    " * We only train model on one fixed region. Try using several regions at once to get more training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ```\n",
    " \n",
    " ```\n",
    "\n",
    "![img](https://images-na.ssl-images-amazon.com/images/I/31la29lBQxL.jpg)\n",
    "\n",
    "\n",
    " ```\n",
    " \n",
    " ```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
